import numpy as np
import onnx
import os
import glob
import helpers.helper_funcs as helpers
import warnings
from onnx_tf.backend import prepare

from onnx import numpy_helper

import tensorflow as tf

def imagenet_postprocess(scores): 
    '''
    Source: https://github.com/onnx/models/blob/master/vision/classification/imagenet_postprocess.py

    Postprocessing with mxnet gluon
    The function takes scores generated by the network and returns the class IDs in decreasing order
    of probability
    '''
    prob = tf.nn.softmax(scores).numpy()
    prob = np.squeeze(prob)
    a = np.argsort(prob)[::-1]
    return a

def imagenet_preprocess(img_data):
    '''
    Source: https://github.com/onnx/models/blob/master/vision/classification/imagenet_preprocess.py
    '''
    mean_vec = np.array([0.485, 0.456, 0.406])
    stddev_vec = np.array([0.229, 0.224, 0.225])
    norm_img_data = np.zeros(img_data.shape).astype('float32')
    for i in range(img_data.shape[0]):  
         # for each pixel in each channel, divide the value by 255 to get value between [0, 1] and then normalize
        norm_img_data[i,:,:] = (img_data[i,:,:]/255 - mean_vec[i]) / stddev_vec[i]
    return norm_img_data

def mobilenet():
    return prepare(onnx.load('onnx_models/mobilenetv2-1.0/mobilenetv2-1.0.onnx'))

def resnet():
    return prepare(onnx.load('onnx_models/resnet18v1/resnet18v1.onnx'))

def squeezenet():
    return prepare(onnx.load('onnx_models/squeezenet1.1/squeezenet1.1.onnx'))

def vggnet():
    return prepare(onnx.load('onnx_models/vgg16/vgg16.onnx'))

def alexnet():
    return prepare(onnx.load('onnx_models/bvlc_alexnet/model.onnx'))

def run_model(tf_rep, test_data_dir='onnx_models/mobilenetv2-1.0/test_data_set_0'):
    # Load inputs
    inputs = []
    inputs_num = len(glob.glob(os.path.join(test_data_dir, 'input_*.pb')))
    for i in range(inputs_num):
        input_file = os.path.join(test_data_dir, 'input_{}.pb'.format(i))
        tensor = onnx.TensorProto()
        with open(input_file, 'rb') as f:
            tensor.ParseFromString(f.read())
        inputs.append(numpy_helper.to_array(tensor))

    # Load reference outputs
    ref_outputs = []
    ref_outputs_num = len(glob.glob(os.path.join(test_data_dir, 'output_*.pb')))
    for i in range(ref_outputs_num):
        output_file = os.path.join(test_data_dir, 'output_{}.pb'.format(i))
        tensor = onnx.TensorProto()
        with open(output_file, 'rb') as f:
            tensor.ParseFromString(f.read())
        ref_outputs.append(numpy_helper.to_array(tensor))

    # Run the model on the backend
    outputs = tf_rep.run(inputs)[0]

    outputs = imagenet_postprocess(outputs)
    # print(outputs[:10,:])
    print(outputs.shape)
    # print(outputs[0,0])
    print(outputs[:10])

    # Compare the results with reference outputs.
    # i = 0
    # for ref_o, o in zip(ref_outputs, outputs):
    #     print(i, ref_o.shape, o.shape)
    #     i = i + 1
        # print(np.allclose(ref_o, o, atol=1e-3, rtol=1e-3))
        # np.testing.assert_almost_equal(ref_o, o, decimal=5)

def get_data():
    '''
    Combine all input_*.pb files and output_*.pb files into a 
    dictionary with key being the "model.test_dir_number_x" and 
    the value being a list of length 2. The first value is the 
    input and the second value is the output.
    '''
    dirToInputOutput = {}

    input_files = glob.glob('onnx_models/*/*/input_*.pb')
    for input_file in input_files:
        tensor = onnx.TensorProto()
        with open(input_file, 'rb') as f:
            tensor.ParseFromString(f.read())
        parsed = numpy_helper.to_array(tensor)

        split_result = input_file.split('/')
        key = split_result[1] + '.' + split_result[2]

        dirToInputOutput[key] = [parsed]

    output_files = glob.glob('onnx_models/*/*/output_*.pb')
    for output_file in output_files:
        tensor = onnx.TensorProto()
        with open(output_file, 'rb') as f:
            tensor.ParseFromString(f.read())
        parsed = numpy_helper.to_array(tensor)


        split_result = output_file.split('/')
        key = split_result[1] + '.' + split_result[2]

        if not key in dirToInputOutput.keys():
            print("ERROR: %v not in dictionary", output_file)
            break

        dirToInputOutput[key].append(parsed)

    return dirToInputOutput

def get_optimal_conf_values(models, time_constraint, conf_value_inputs, conf_value_labels):
    num_conf_vals = len(models) - 1

    conf_values = np.zeros(num_conf_vals)

    _, best_conf_values = get_optimal_conf_values_helper(models, conf_values, \
                            0, time_constraint, conf_value_inputs, conf_value_labels)

    return best_conf_values
   
def get_optimal_conf_values_helper(models, conf_values, curr_index, time_constraint,\
                                     conf_value_inputs, conf_value_labels):
    '''
    Return array of optimal confidence values
    '''
    if curr_index == len(conf_values):
        # Run models with conf_values and return accuracy, none if doesn't satisfy time constraint
        # also return corresponding conf values
        accuracy, time = run(models, conf_values)
        if time < time_constraint:
            return accuracy, np.copy(conf_values) # TODO: think if there is any way to get rid of copy
        else:
            return None, None
    else:
        best_accuracy = 0
        best_conf_values = None

        for conf_value in np.arange(0, 1.1, 0.1):
            conf_values[curr_index] = conf_value
            accuracy, conf_values = get_optimal_conf_values(models, conf_values, \
                                                            curr_index + 1, time_constraint)
            if not accuracy == None and accuracy > best_accuracy:
                best_conf_values = conf_values
                best_accuracy = accuracy

        return best_accuracy, best_conf_values

def get_all_model_combinations(n):
    pass

def run(models, conf_values):
    assert len(models) - 1 == conf_values
    pass

def naive_search(models, validation_inputs, validation_labels,\
                 conf_value_inputs, conf_value_labels, input_time_constraint):
    '''
    Given list of onnx models, find combination of models that satisfied input time constraint

    Conf_value data is used to find the optimal confidence values for a set of models.

    Validation data used to validate optimal confidence value found - the accuracy and time
    of the validation data is used to determine the best model combination.
    '''

    # Enumerate all possible model combinations (n!) - use indices
    all_model_combinations = get_all_model_combinations(len(models))

    best_combination = None
    best_accuracy = 0

    # Loop through each model combination to find the optimal confidence value for each.
    for model_combination in all_model_combinations:
        input_models = np.take(model_combination, models)
        conf_values = get_optimal_conf_values(input_models)

        # Test model combination with optimal confidence values    
        accuracy, _ = run(input_models, conf_values)
        if accuracy > best_accuracy:
            best_combination = model_combination

    # Record best model combination
    return best_combination

def main():
    warnings.filterwarnings('ignore') # Ignore all the warning messages 

    get_data()

    # run_model(alexnet())
    # run_model(vggnet())
    # run_model(squeezenet())
    # run_model(resnet())
    # run_model(mobilenet())

if __name__ == '__main__':
    main()
